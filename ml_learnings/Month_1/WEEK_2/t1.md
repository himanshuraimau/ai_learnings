### **Simple Linear Regression**

1. **Description**:
    - Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (target). It assumes a linear relationship between the predictor and the target.
    - The goal is to find the best-fitting line (usually a straight line) that minimizes the sum of squared differences between the observed and predicted values.

2. **Equation**:
    - The equation for simple linear regression is: 
      ```
      y = mx + b
      ```
      where:
        - y is the dependent variable (target),
        - x is the independent variable (predictor),
        - m is the slope of the line (the coefficient of x),
        - b is the y-intercept.

3. **Example**:
    - Predicting house prices (y) based on the size of the house (x). Here, the size of the house is the only predictor.

### **Multiple Linear Regression**

1. **Description**:
    - Multiple linear regression extends simple linear regression to model the relationship between multiple independent variables (predictors) and a single dependent variable (target). It assumes a linear relationship between the predictors and the target.
    - The goal is to find the best-fitting hyperplane that minimizes the sum of squared differences between the observed and predicted values.

2. **Equation**:
    - The equation for multiple linear regression is: 
      ```
      y = b0 + b1*x1 + b2*x2 + ... + bn*xn
      ```
      where:
        - y is the dependent variable (target),
        - x1, x2, ..., xn are the independent variables (predictors),
        - b0 is the y-intercept,
        - b1, b2, ..., bn are the coefficients for each predictor.

3. **Example**:
    - Predicting house prices (y) based on multiple features such as size, number of bedrooms, location, etc. Here, size, number of bedrooms, and location are predictors.




### **Cost Function**

1. **Description**:
    - The cost function (also known as the loss function or objective function) measures the difference between the predicted values of a machine learning model and the actual values (labels) in the training data.
    - It quantifies how well or poorly the model is performing on the training data.
    - The goal is to minimize the value of the cost function, indicating that the model's predictions are as close to the actual values as possible.

2. **Example**:
    - In linear regression, the commonly used cost function is the Mean Squared Error (MSE) function, which calculates the average of the squared differences between the predicted values and the actual values:
      ```
      MSE = (1 / m) * Σ(y_pred - y_actual)^2
      ```
      where:
        - m is the number of training examples,
        - y_pred is the predicted value,
        - y_actual is the actual value.

### **Gradient Descent**

1. **Description**:
    - Gradient descent is an optimization algorithm used to minimize the cost function by iteratively adjusting the parameters (weights and biases) of the machine learning model.
    - It works by calculating the gradient (derivative) of the cost function with respect to each parameter, indicating the direction of steepest ascent.
    - The parameters are then updated in the opposite direction of the gradient, moving towards the minimum of the cost function.

2. **Algorithm**:
    - Update rule for parameter θ_j:
      ```
      θ_j := θ_j - α * ∂(cost function) / ∂(θ_j)
      ```
      where:
        - α (alpha) is the learning rate, determining the size of the step taken in each iteration,
        - ∂(cost function) / ∂(θ_j) is the partial derivative of the cost function with respect to parameter θ_j.

3. **Example**:
    - In linear regression, gradient descent is used to update the parameters (slope and intercept) of the regression line to minimize the MSE cost function and improve the accuracy of predictions.
